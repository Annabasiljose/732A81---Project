{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cff5f33c-fe5c-4849-82b2-7c72419d6b8c",
      "metadata": {
        "id": "cff5f33c-fe5c-4849-82b2-7c72419d6b8c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "94adf39b-4772-4a42-92d9-52e010ecf2bd",
      "metadata": {
        "id": "94adf39b-4772-4a42-92d9-52e010ecf2bd"
      },
      "outputs": [],
      "source": [
        "# Load the CSV files\n",
        "data = pd.read_csv('training_backup.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40aa39b-7794-4065-8bd2-18708b616d0e",
      "metadata": {
        "id": "e40aa39b-7794-4065-8bd2-18708b616d0e"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove numbers and non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    lemmatized = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(lemmatized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59278297-e0fe-4e3b-8969-1f090832d4a0",
      "metadata": {
        "id": "59278297-e0fe-4e3b-8969-1f090832d4a0"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to the text\n",
        "data['lyrics'] = data['lyrics'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e5a1d08-7ace-407c-b1d9-5b11838fb9d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e5a1d08-7ace-407c-b1d9-5b11838fb9d0",
        "outputId": "a0c3d381-e074-4eda-91c2-f5e39c0ac974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from typing import List\n",
        "import torch\n",
        "\n",
        "def back_translate(texts: List[str], src_lang: str, tgt_lang: str, max_length: int) -> List[str]:\n",
        "    # Initialize tokenizers and models\n",
        "    tokenizer_tgt = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}')\n",
        "    model_tgt = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}')\n",
        "    tokenizer_src = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}')\n",
        "    model_src = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}')\n",
        "\n",
        "    # Function to translate text and handle max_length\n",
        "    def translate(text, tokenizer, model):\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "        with torch.no_grad():\n",
        "            translated = model.generate(**inputs)\n",
        "        return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "    # Translate to the target language and back\n",
        "    src_texts = []\n",
        "    for text in texts:\n",
        "        # Translate to target language\n",
        "        tgt_text = translate(text, tokenizer_tgt, model_tgt)\n",
        "        # Back-translate to source language\n",
        "        back_translated_text = translate(tgt_text, tokenizer_src, model_src)\n",
        "        src_texts.append(back_translated_text)\n",
        "\n",
        "    return src_texts\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('training_backup.csv')\n",
        "\n",
        "# Select the minority class lyrics\n",
        "minority_lyrics = df[df['mood'] == 4]['lyrics'].tolist()\n",
        "\n",
        "# Perform back-translation\n",
        "augmented_lyrics = back_translate(minority_lyrics[:100], 'en', 'fr',512)  # Example with English to French and back to English\n",
        "\n",
        "# Create a new DataFrame with the augmented data\n",
        "augmented_df = pd.DataFrame({'lyrics': augmented_lyrics, 'mood': 4})\n",
        "\n",
        "# Combine the original dataset with the augmented data\n",
        "augmented_dataset = pd.concat([df, augmented_df])\n",
        "\n",
        "# Save the augmented dataset\n",
        "augmented_df.to_csv('augmented_lyrics_mood.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eA261aslu5r"
      },
      "id": "4eA261aslu5r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}